{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La detección de objetos es una tarea de visión por computadora en la que el objetivo es detectar y localizar objetos de interés en una imagen o vídeo. La tarea consiste en identificar la posición y los límites de los objetos en una imagen y clasificarlos en diferentes categorías. Forma una parte crucial del reconocimiento de la visión, junto con la clasificación y recuperación de imágenes.\n",
    "\n",
    "![OD](https://miro.medium.com/v2/resize:fit:828/format:webp/1*IrptRDRG8IL9o-55BKjbLA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las Bounding Boxes son rectángulos que se dibujan alrededor de los objetos de interés en una imagen. Estos cuadros se utilizan para representar la posición de un objeto en una imagen. Existen varias formas para delimitar los objetos, pero las más comun es calcular 2 puntos X,Y en la esquina superior izquierda y en la esquina inferior derecha respectivamente.\n",
    "\n",
    "![BB](https://miro.medium.com/v2/resize:fit:640/format:webp/1*yvPdthNNOJfmcYKsDhJJtQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Dataset utilizado será Caltech101 [https://data.caltech.edu/records/mzrjq-6wc02] que contiene imágenes de 101 categorías de objetos (por ejemplo, \"helicóptero\", \"elefante\" y \"silla\", etc.) y una categoría de fondo que contiene imágenes que no pertenecen a las 101 categorías de objetos. Para cada categoría de objeto, hay entre 40 y 800 imágenes, mientras que la mayoría de las clases tienen alrededor de 50 imágenes. La resolución de la imagen es aproximadamente de 300×200 píxeles.\n",
    "\n",
    "Este Dataset tiene incluida las anotaciones de las bounding Boxes de cada imagen, por lo que sera bastante fácil poder implementar un modelo de detección de objetos.\n",
    "\n",
    "![img_caltech](caltech-101/101_ObjectCategories/airplanes/image_0005.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import scipy.io\n",
    "import matplotlib.patches as patches\n",
    "import keras_core as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path de las imágenes y anotaciones\n",
    "path_images = \"caltech-101/101_ObjectCategories/airplanes/\"\n",
    "path_annot = \"caltech-101/Annotations/Airplanes_Side_2/\"\n",
    "\n",
    "image_size = 224  # tamaño a redimensionar las imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocesamiento de Imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path_images, path_annot, image_size):\n",
    "    \n",
    "    # almacenamos todas las imagenes y sus labels en listas\n",
    "    image_paths = [f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f))]\n",
    "    annot_paths = [f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f))]\n",
    "    image_paths.sort()\n",
    "    annot_paths.sort()\n",
    "\n",
    "    images, targets = [], []\n",
    "\n",
    "    # iteramos sobre las imagenes y sus labels\n",
    "    for i in range(0, len(annot_paths)):\n",
    "\n",
    "        # cargamos las bounding boxes\n",
    "        annot = scipy.io.loadmat(path_annot + annot_paths[i])[\"box_coord\"][0]\n",
    "\n",
    "        # extraemos las coordenadas de las bounding boxes\n",
    "        x1, y1 = annot[2], annot[0]\n",
    "        x2, y2 = annot[3], annot[1]\n",
    "\n",
    "        # cargamos la imagen\n",
    "        image = keras.utils.load_img(path_images + image_paths[i])\n",
    "\n",
    "        # obtenemos el tamaño de la imagen\n",
    "        (w, h) = image.size[:2]\n",
    "\n",
    "        # reescalamos la imagen\n",
    "        image = image.resize((image_size, image_size))\n",
    "\n",
    "        # convertimos la imagen a un array y la almacenamos en la lista\n",
    "        images.append(keras.utils.img_to_array(image))\n",
    "\n",
    "        # normalizamos las coordenadas de las bounding boxes\n",
    "        targets.append(\n",
    "            (\n",
    "                float(x1) / w,\n",
    "                float(y1) / h,\n",
    "                float(x2) / w,\n",
    "                float(y2) / h,\n",
    "            )\n",
    "    )\n",
    "        \n",
    "    # convertimos las listas a arrays para el conjunto de entrenamiento y test (80% y 20% respectivamente)\n",
    "    (x_train), (y_train) = (\n",
    "        np.asarray(images[: int(len(images) * 0.8)]),\n",
    "        np.asarray(targets[: int(len(targets) * 0.8)]),\n",
    "    )\n",
    "\n",
    "    (x_test), (y_test) = (\n",
    "        np.asarray(images[int(len(images) * 0.8) :]),\n",
    "        np.asarray(targets[int(len(targets) * 0.8) :]),\n",
    "    )\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = preprocess_image(path_images, path_annot, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer, o ViT [https://arxiv.org/abs/2010.11929], es un modelo para clasificación de imágenes que emplea una arquitectura similar a Transformer sobre parches de la imagen. Una imagen se divide en parches de tamaño fijo, luego esos parches se proyectan linealmente a través de una capa de embedding, se agregan los embedding de la posición y la secuencia resultante de vectores es el input de nuestro bloque encoder del Transformer. Por ultimo se agrega un cabezal MLP para la clasificacion, pero en este caso en vez de clasificar las imágenes, haremos la prediccion de las bounding boxes.\n",
    "\n",
    "![ViT](https://learnopencv.com/wp-content/uploads/2023/02/image-9-1536x807.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo Notar que ViT se puede utilizar para varios problemas, no solo clasificacion de imagenes como lo presentan en el Paper, en este caso lo estamos utilizando para deteccion de objetos y es por eso que cambiamos la capa final del modelo, para que en vez de tener una capa con activacion softmax para predecir la clase, ahora tenemos una capa con 4 neuronas para predecir las 4 coordenadas de la bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Visualización de Parches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(keras.layers.Layer): #Clase para extraer los parches de las imágenes\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size #Tamaño de los parches\n",
    "\n",
    "    def call(self, images): #Función para extraer los parches de las imágenes\n",
    "        batch_size = tf.shape(images)[0] #Tamaño del batch\n",
    "        patches = tf.image.extract_patches( \n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1], #Tamaño de los parches\n",
    "            strides=[1, self.patch_size, self.patch_size, 1], #Tamaño del salto entre parches\n",
    "            rates=[1, 1, 1, 1], #Tamaño de la ventana\n",
    "            padding=\"VALID\", #Tipo de padding\n",
    "        )\n",
    "\n",
    "        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]]) #Devolvemos los parches como una secuencia para que el modelo pueda procesarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 32  # tamaño de los parches\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(x_train[0].astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "patches = Patches(patch_size)(tf.convert_to_tensor([x_train[0]])) #Utilizamos la clase Patches para extraer los parches de la imagen\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1])) #Número de parches a mostrar\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Patch Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El enconder de los parches es una parte fundamental de nuestro modelo, ya que le da continuidad y orden a los parches de la imagen, si por el contrario no utilizaramos los embeddings de la poscicion de cada parche, el modelo juntaria los parches de manera aleatoria y la imagen resultante no tendria coherencia ni sentido, es por esto que utilizar la informacion de la posicion de cada parche es fundamental para el buen funcionamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(keras.layers.Layer): #Clase para codificar los parches\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches                              #Número de parches\n",
    "        self.projection = keras.layers.Dense(units=projection_dim)  #Capa densa para codificar los parches\n",
    "        self.position_embedding = keras.layers.Embedding(input_dim=num_patches, output_dim=projection_dim) #Capa de embedding para codificar la posición de los parches\n",
    "\n",
    "    def call(self, patch): #Función para codificar los parches\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1) #Creamos un vector con la posición de los parches\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions) #Concatemanos los embeddings de los parches con los embeddings de su posición\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate): #Función para crear la capa MLP\n",
    "\n",
    "    #Iteramos sobre las unidades de la capa\n",
    "    for units in hidden_units:                \n",
    "        #Añadimos una capa densa con activación gelu\n",
    "        x = keras.layers.Dense(units, activation='gelu', kernel_initializer=\"he_normal\")(x) # completao ?\n",
    "        #Añadimos una capa de dropout\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta: Que beneficios tiene GeLU por sobre ReLU?**\n",
    "\n",
    "**Respuesta:** GeLU es diferenciable en todo punto, no asi relu que no es diferenciable en el punto 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4 ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViT(input_shape,patch_size,num_patches,projection_dim,num_heads,transformer_units,transformer_layers,mlp_head_units):\n",
    "\n",
    "    inputs = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Creamos los parches\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "\n",
    "    # Codificamos los parches\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Creamos n capas de encoders\n",
    "    for _ in range(transformer_layers):\n",
    "\n",
    "        # Layer normalization 1.\n",
    "        x1 = keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "        #multi-head self-attention layer.\n",
    "        attention_output = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = keras.layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = keras.layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "        # MLP\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)      \n",
    "        # Skip connection 2\n",
    "        encoded_patches = keras.layers.Add()([x3, x2])\n",
    "\n",
    "\n",
    "    # Hacemos un Flatten de los parches codificados para pasarlos por la capa MLP Final\n",
    "    representation = keras.layers.LayerNormalization()(encoded_patches)\n",
    "    representation = keras.layers.Flatten()(representation)\n",
    "    representation = keras.layers.Dropout(0.2)(representation)\n",
    "\n",
    "    # MLP final Head\n",
    "    \n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.2)\n",
    "    \n",
    "    bounding_box = keras.layers.Dense(4)(features)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=bounding_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "input_shape = (image_size, image_size, 3)\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]\n",
    "\n",
    "transformer_layers = 4\n",
    "mlp_head_units = [2048, 1024, 512, 64, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 Creamos el Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    num_patches,\n",
    "    projection_dim,\n",
    "    num_heads,\n",
    "    transformer_units,\n",
    "    transformer_layers,\n",
    "    mlp_head_units,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3 Agregamos el optimizador y la funcion de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el optimizador AdamW\n",
    "adamw_opt = keras.optimizers.AdamW(learning_rate=1e-4)\n",
    "\n",
    "# Define el error cuadrático medio como función de pérdida\n",
    "mse_loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(optimizer=adamw_opt, loss=mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta: Que Busca Solucionar AdamW de Adam?**\n",
    "\n",
    "**Respuesta:** Busca solucionar la regularización de los pesos en una gradiente. Esto lo hace modificando el decaimiento de los pesos en una gradiente, teniendo una mejora en rendimiento en comparación con Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.4 Creamos nuestro callback de Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "      monitor=\"val_loss\",\n",
    "      min_delta=0,\n",
    "      patience=10,\n",
    "      restore_best_weights=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.5 Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs, validation_split=0.1, callbacks=es_callback,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history, loss):\n",
    "  \"Función para graficar la función de pérdida del modelo\"\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Error Graph')\n",
    "  plt.ylabel(loss.name)\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "plot_losses(history, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 IoU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection Over Union (IoU) es una métrica de evaluación común para la detección de objetos. Se utiliza para medir la superposición entre el área predicha y el área de destino. La superposición se calcula como el área de intersección entre las áreas predichas y de destino dividida por el área de unión entre las áreas predichas y de destino.\n",
    "\n",
    "![Foto](https://www.baeldung.com/wp-content/uploads/sites/4/2022/04/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.1 Función de IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box_intersection_over_union(box_predicted, box_truth): #Función para calcular el IoU entre dos bounding boxes\n",
    "\n",
    "    # obtenemos las coordenadas (x, y) de la intereseccion de las bounding boxes\n",
    "    top_x_intersect = max(box_predicted[0], box_truth[0])\n",
    "    top_y_intersect = max(box_predicted[1], box_truth[1])\n",
    "    bottom_x_intersect = min(box_predicted[2], box_truth[2])\n",
    "    bottom_y_intersect = min(box_predicted[3], box_truth[3])\n",
    "\n",
    "    # calculamos el area de la intereseccion\n",
    "    intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(0, bottom_y_intersect - top_y_intersect + 1)\n",
    "\n",
    "    # calculamos el area de las bounding boxe predicha y real\n",
    "    box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (box_predicted[3] - box_predicted[1] + 1)\n",
    "    box_truth_area = (box_truth[2] - box_truth[0] + 1) * (box_truth[3] - box_truth[1] + 1)\n",
    "\n",
    "    # Calculamos el IoU dividiendo el area de la interseccion por la suma del area de las bounding boxes predicha y real menos el area de la interseccion\n",
    "    IoU = intersection_area/(box_predicted_area + box_truth_area - intersection_area)\n",
    "\n",
    "    # retornamos el IoU\n",
    "    return IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Carga de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocesamiento de Imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2 Visualización de IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "max_test_imgs = 10\n",
    "i, mean_iou = 0, 0\n",
    "\n",
    "# Iteramos sobre las imágenes de test\n",
    "for input_image in x_test[:max_test_imgs]:\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    im = input_image\n",
    "\n",
    "    # Mostramos la imagen original\n",
    "    ax.imshow(im.astype(\"uint8\"))\n",
    "\n",
    "    # Redimensionamos la imagen\n",
    "    input_image = cv2.resize(input_image, (image_size, image_size), interpolation=cv2.INTER_AREA)\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "    # Predecimos las coordenadas de la bounding box\n",
    "    preds = model.predict(input_image)[0]\n",
    "    (h, w) = im.shape[0:2]\n",
    "    top_left_x_pred, top_left_y_pred = int(preds[0] * w), int(preds[1] * h)\n",
    "    bottom_right_x_pred, bottom_right_y_pred = int(preds[2] * w), int(preds[3] * h)\n",
    "    box_predicted = [top_left_x_pred, top_left_y_pred, bottom_right_x_pred, bottom_right_y_pred]\n",
    "\n",
    "    # Creamos el rectángulo de la bounding box predicha\n",
    "    rect_pred = patches.Rectangle(\n",
    "        (top_left_x_pred, top_left_y_pred),\n",
    "        bottom_right_x_pred - top_left_x_pred,\n",
    "        bottom_right_y_pred - top_left_y_pred,\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"red\",\n",
    "        linewidth=1,\n",
    "    )\n",
    "    ax.add_patch(rect_pred)\n",
    "\n",
    "    # Obtenemos las coordenadas de la bounding box real\n",
    "    top_left_x_true, top_left_y_true = int(y_test[i][0] * w), int(y_test[i][1] * h)\n",
    "    bottom_right_x_true, bottom_right_y_true = int(y_test[i][2] * w), int(y_test[i][3] * h)\n",
    "    box_truth = [top_left_x_true, top_left_y_true, bottom_right_x_true, bottom_right_y_true]\n",
    "\n",
    "    # Creamos el rectángulo de la bounding box real\n",
    "    rect_true = patches.Rectangle(\n",
    "        (top_left_x_true, top_left_y_true),\n",
    "        bottom_right_x_true - top_left_x_true,\n",
    "        bottom_right_y_true - top_left_y_true,\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"blue\",\n",
    "        linewidth=1,\n",
    "    )\n",
    "    ax.add_patch(rect_true)\n",
    "\n",
    "    # Calculamos el IoU entre la bounding box predicha y la real\n",
    "    iou_value = bounding_box_intersection_over_union(box_predicted, box_truth)\n",
    "    mean_iou += iou_value\n",
    "\n",
    "    ax.set_xlabel(\n",
    "        \"Predicted (Red): \"\n",
    "        + str(box_predicted)\n",
    "        + \"\\nTarget (Blue): \"\n",
    "        + str(box_truth)\n",
    "        + \"\\nIoU: \"\n",
    "        + str(iou_value)\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "# Mostramos el IoU medio\n",
    "print(\"mean_iou: \" + str(mean_iou / len(x_test[:10])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca podemos ver en rojo la bounding box predicha y en Azul la bounding box real, y en la parte Inferior podemos ver el valor de IoU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Segmentación semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segmentación semántica es una tarea de visión por computadora en la que el objetivo es categorizar cada píxel de una imagen en una clase u objeto. El objetivo es producir un mapa de segmentación de píxeles de una imagen, donde cada píxel se asigna a una clase u objeto específico. Los modelos generalmente se evalúan con las métricas Mean Intersection-Over-Union (Mean IoU) y Precisión de píxeles.\n",
    "\n",
    "![SS](https://uploads-ssl.webflow.com/614c82ed388d53640613982e/63f498f8d4fe7da3b3a60cc2_semantic%20segmentation%20vs%20instance%20segmentation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cityscapes [https://www.cityscapes-dataset.com/] es un dataset etiquetado de videos de automoviles conduciendo en Alemania. Esta versión es una muestra preprocesada creada como parte del paper Pix2Pix. El conjunto de datos tiene imágenes de los videos originales y las etiquetas de segmentación se muestran junto a la imagen original. Este es uno de los mejores conjuntos de datos que existen para tareas de segmentación semántica.\n",
    "\n",
    "Este conjunto de datos tiene 2975 archivos de imágenes de entrenamiento y 500 archivos de imágenes de validación. Cada archivo de imagen tiene 256x512 píxeles y cada archivo es una composición con la foto original en la mitad izquierda de la imagen, junto con la imagen etiquetada (salida de la segmentación semántica) en la mitad derecha.\n",
    "\n",
    "Cuenta con anotaciones de píxeles semánticas para 30 clases agrupadas en 8 categorías (superficies planas, humanos, vehículos, construcciones, objetos, naturaleza, cielo y vacío).\n",
    "\n",
    "![Foto](cityscapes_data/val/1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnjNkSJKkhZU"
   },
   "source": [
    "### 2.1 Librerias Utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-21T08:07:08.019200Z",
     "iopub.status.busy": "2022-09-21T08:07:08.018775Z",
     "iopub.status.idle": "2022-09-21T08:07:08.348868Z",
     "shell.execute_reply": "2022-09-21T08:07:08.347859Z",
     "shell.execute_reply.started": "2022-09-21T08:07:08.019163Z"
    },
    "id": "BvMx4krDkhZU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es46xAgikhZV"
   },
   "outputs": [],
   "source": [
    "# Si desea preprocesar las imagenes en local utilize estas funciones, de lo contrario puede utilizar los tensores que vienen en la carpeta cityscapes_data/npy/\n",
    "\n",
    "# Funciones para procesar las imágenes desde el archivo image_processing.py\n",
    "from image_processing import process_image, process_image_batch \n",
    "\n",
    "# Librería para paralelizar el procesamiento de las imágenes\n",
    "import multiprocessing                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Funciones de ayuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T02:46:25.342916Z",
     "iopub.status.busy": "2022-09-21T02:46:25.342290Z",
     "iopub.status.idle": "2022-09-21T02:46:25.352608Z",
     "shell.execute_reply": "2022-09-21T02:46:25.351617Z",
     "shell.execute_reply.started": "2022-09-21T02:46:25.342878Z"
    },
    "id": "cnpEA_clkhZV"
   },
   "outputs": [],
   "source": [
    "def preprocess(path, id_map): # Función para preprocesar las imágenes\n",
    "\n",
    "    # Cargamos la imagen\n",
    "    img = Image.open(path) \n",
    "\n",
    "    # Separamos la imagen de la mascara y la redimensionamos\n",
    "    img1 = img.crop((0, 0, 256, 256)).resize((128,128))\n",
    "    img2 = img.crop((256, 0, 512, 256)).resize((128,128))\n",
    "\n",
    "    # normalizamos la imagen\n",
    "    img1 = np.array(img1) / 255.\n",
    "    img2 = np.array(img2)\n",
    "\n",
    "    # Creamos una máscara vacía\n",
    "    mask = np.zeros(shape=(img2.shape[0], img2.shape[1]), dtype = np.uint32) \n",
    "\n",
    "    # Iteramos sobre la imagen para asignar un id a cada color de la mascara\n",
    "    for row in range(img2.shape[0]): \n",
    "        for col in range(img2.shape[1]):\n",
    "            a = img2[row, col, :]\n",
    "            final_key = None\n",
    "            final_d = None\n",
    "            for key, value in id_map.items():\n",
    "                d = np.sum(np.sqrt(pow(a - value, 2)))\n",
    "                if final_key == None:\n",
    "                    final_d = d\n",
    "                    final_key = key\n",
    "                elif d < final_d:\n",
    "                    final_d = d\n",
    "                    final_key = key\n",
    "            mask[row, col] = final_key\n",
    "\n",
    "    # Añadimos una dimensión a la máscara para poder utilizarla de manera correcta en el modelo\n",
    "    mask = np.reshape(mask, (mask.shape[0], mask.shape[1], 1)) \n",
    "\n",
    "    del img2 # Eliminamos la imagen de la máscara para liberar memoria\n",
    "\n",
    "    # Devolvemos la imagen y la máscara\n",
    "    return img1, mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aIxbCl4khZW"
   },
   "outputs": [],
   "source": [
    "# Función principal para preparar el conjunto de datos\n",
    "def prepare_tensor_dataset(train_path, val_path):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    # Procesa las imágenes de entrenamiento en paralelo\n",
    "    train_files = [os.path.join(train_path, file) for file in os.listdir(train_path)]\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=num_cores)\n",
    "    results_train = pool.map(process_image_batch, np.array_split(train_files, num_cores))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Guardamos las imágenes y las máscaras en listas\n",
    "    for result_batch in results_train:\n",
    "        for img, mask in result_batch:\n",
    "            X_train.append(img)\n",
    "            Y_train.append(mask)\n",
    "\n",
    "    # Procesa las imágenes de validación en paralelo\n",
    "    val_files = [os.path.join(val_path, file) for file in os.listdir(val_path)]\n",
    "    pool = multiprocessing.Pool(processes=num_cores)\n",
    "    results_val = pool.map(process_image_batch, np.array_split(val_files, num_cores))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Guardamos las imágenes y las máscaras en listas\n",
    "    for result_batch in results_val:\n",
    "        for img, mask in result_batch:\n",
    "            X_val.append(img)\n",
    "            Y_val.append(mask)\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mapa de Segmentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T08:07:45.012579Z",
     "iopub.status.busy": "2022-09-21T08:07:45.012128Z",
     "iopub.status.idle": "2022-09-21T08:07:45.025882Z",
     "shell.execute_reply": "2022-09-21T08:07:45.024553Z",
     "shell.execute_reply.started": "2022-09-21T08:07:45.012541Z"
    },
    "id": "-eZNFDWnkhZV"
   },
   "outputs": [],
   "source": [
    "id_map = { \n",
    "    0: (0, 0, 0), # unlabelled\n",
    "    1: (111, 74,  0), #static\n",
    "    2: ( 81,  0, 81), #ground\n",
    "    3: (128, 64,127), #road\n",
    "    4: (244, 35,232), #sidewalk\n",
    "    5: (250,170,160), #parking\n",
    "    6: (230,150,140), #rail track\n",
    "    7: (70, 70, 70), #building\n",
    "    8: (102,102,156), #wall\n",
    "    9: (190,153,153), #fence\n",
    "    10: (180,165,180), #guard rail\n",
    "    11: (150,100,100), #bridge\n",
    "    12: (150,120, 90), #tunnel\n",
    "    13: (153,153,153), #pole\n",
    "    14: (153,153,153), #polegroup\n",
    "    15: (250,170, 30), #traffic light\n",
    "    16: (220,220,  0), #traffic sign\n",
    "    17: (107,142, 35), #vegetation\n",
    "    18: (152,251,152), #terrain\n",
    "    19: ( 70,130,180), #sky\n",
    "    20: (220, 20, 60), #person\n",
    "    21: (255,  0,  0), #rider\n",
    "    22: (  0,  0,142), #car\n",
    "    23: (  0,  0, 70), #truck\n",
    "    24: (  0, 60,100), #bus\n",
    "    25: (  0,  0, 90), #caravan\n",
    "    26: (  0,  0,110), #trailer\n",
    "    27: (  0, 80,100), #train\n",
    "    28: (  0,  0,230), #motorcycle\n",
    "    29: (119, 11, 32), #bicycle\n",
    "    30: (  0,  0,142) #license plate\n",
    "}\n",
    "\n",
    "category_map = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 1,\n",
    "    6: 1,\n",
    "    7: 2,\n",
    "    8: 2,\n",
    "    9: 2,\n",
    "    10: 2,\n",
    "    11: 2,\n",
    "    12: 2,\n",
    "    13: 3,\n",
    "    14: 3,\n",
    "    15: 3,\n",
    "    16: 3,\n",
    "    17: 4,\n",
    "    18: 4,\n",
    "    19: 5,\n",
    "    20: 6,\n",
    "    21: 6,\n",
    "    22: 7,\n",
    "    23: 7,\n",
    "    24: 7,\n",
    "    25: 7,\n",
    "    26: 7,\n",
    "    27: 7,\n",
    "    28: 7,\n",
    "    29: 7,\n",
    "    30: 7\n",
    "}\n",
    "\n",
    "# Número de clases\n",
    "num_classes = len(id_map.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Procesamiento de las Imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si va a preprocesar las imagenes en local corra las siguientes celdas, sino puede saltar a la celda 1.4.3 y cargar los tensores directamente ya que el preprocesamiento demora bastante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "execution": {
     "iopub.execute_input": "2022-09-21T02:46:25.640040Z",
     "iopub.status.busy": "2022-09-21T02:46:25.639289Z",
     "iopub.status.idle": "2022-09-21T07:15:57.898850Z",
     "shell.execute_reply": "2022-09-21T07:15:57.897706Z",
     "shell.execute_reply.started": "2022-09-21T02:46:25.640003Z"
    },
    "id": "HdaPfWP0khZW",
    "outputId": "71a7cb30-ca7b-49dc-9648-5a9d02070af8"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_valid, Y_valid = prepare_tensor_dataset(\"cityscapes_data/train/\", \"cityscapes_data/val/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T07:31:27.302290Z",
     "iopub.status.busy": "2022-09-21T07:31:27.301797Z",
     "iopub.status.idle": "2022-09-21T07:31:27.779621Z",
     "shell.execute_reply": "2022-09-21T07:31:27.778616Z",
     "shell.execute_reply.started": "2022-09-21T07:31:27.302249Z"
    },
    "id": "aep4j0UKkhZW"
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Guardamos los tensores para no tener que volver a procesarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cityscapes_data/npy/X_train.npy', X_train)\n",
    "np.save('cityscapes_data/npy/Y_train.npy', Y_train)\n",
    "np.save('cityscapes_data/npy/X_valid.npy', X_valid)\n",
    "np.save('cityscapes_data/npy/Y_valid.npy', Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Cargamos los tensores preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('npy/X_train.npy')\n",
    "Y_train = np.load('npy/Y_train.npy')\n",
    "X_valid = np.load('npy/X_valid.npy')\n",
    "Y_valid = np.load('npy/Y_valid.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Metricas y Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdatedMeanIoU(tf.keras.metrics.MeanIoU): # Clase para calcular el Mean IoU\n",
    "  def __init__(self, y_true=None, y_pred=None, num_classes=None, name=None, dtype=None):\n",
    "    super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None): # Función para actualizar el estado del Mean IoU\n",
    "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "    return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "class VizCallback(tf.keras.callbacks.Callback): # Clase para visualizar las predicciones del modelo\n",
    "\n",
    "    def __init__(self, file_path, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): # Función para visualizar las predicciones del modelo al final de cada época\n",
    "        #plt.style.use(\"default\")\n",
    "        img, mask = preprocess(self.file_path, id_map)\n",
    "        img = np.array(img)\n",
    "        img = np.reshape(img, (1, 128, 128, 3))\n",
    "        pred = model.predict(img)\n",
    "        y_pred = tf.math.argmax(pred, axis=-1)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_pred = np.reshape(y_pred, (128, 128))\n",
    "        fig, axes = plt.subplots(nrows = 1, ncols = 2)\n",
    "        axes[0].imshow(mask)\n",
    "        axes[0].set_title(\"Original Mask\")\n",
    "        axes[1].imshow(y_pred)\n",
    "        axes[1].set_title(\"Predicted Mask\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_history(history): # Función para visualizar las métricas de entrenamiento y validación\n",
    "  fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize=(20, 7))\n",
    "  sns.lineplot\n",
    "\n",
    "  #training\n",
    "  sns.lineplot(history.history[\"loss\"], ax = axes[0], label=\"Training Loss\")\n",
    "  sns.lineplot(history.history[\"accuracy\"], ax = axes[1], label=\"Training Accuracy\")\n",
    "  sns.lineplot(history.history[\"mean_iou\"], ax = axes[2], label=\"Training Mean IOU\")\n",
    "\n",
    "  # Validacion\n",
    "  sns.lineplot(history.history[\"val_loss\"], ax = axes[0], label=\"Validation Loss\")\n",
    "  sns.lineplot(history.history[\"val_accuracy\"], ax = axes[1], label=\"Validation Accuracy\")\n",
    "  sns.lineplot(history.history[\"val_mean_iou\"], ax = axes[2], label=\"Validation Mean IOU\")\n",
    "\n",
    "  axes[0].set_title(\"Loss Comparison\", fontdict = {'fontsize': 15})\n",
    "  axes[0].set_xlabel(\"Epoch\")\n",
    "  axes[0].set_ylabel(\"Loss\")\n",
    "\n",
    "  axes[1].set_title(\"Accuracy Comparison\", fontdict = {'fontsize': 15})\n",
    "  axes[1].set_xlabel(\"Epoch\")\n",
    "  axes[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "  axes[2].set_title(\"Mean IOU Comparison\", fontdict = {'fontsize': 15})\n",
    "  axes[2].set_xlabel(\"Epoch\")\n",
    "  axes[2].set_ylabel(\"Mean IOU\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84tRNyEfkhZX"
   },
   "source": [
    "### 2.6 Modelos para Segmentación Semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se presentaran 2 modelos para segmentacion Semantica, el primero ya esta implementado y sirve como ejemplo de prueba, ya que para el segundo modelo ustedes deberan completar el modelo y asegurarse de que funcione de manera correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1 PSPnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSPNet [https://www.computer.org/csdl/proceedings-article/cvpr/2017/0457g230/12OmNvrMUeP], o Pyramid Scene Parsing Network, es un modelo de segmentación semántica que utiliza un módulo de parsing piramidal.\n",
    "\n",
    "Dada una imagen de entrada, PSPNet utiliza una CNN dilatada previamente entrenada para extraer los feature maps. El tamaño final feature map es 1/8 de la imagen de entrada. En la parte superior del feature map, utilizamos un modulo de pooling piramidal (pyramid pooling module) para recopilar información de contexto. \n",
    "Usando una pirámide de 4 niveles, los kernels cubren la totalidad, la mitad y pequeñas porciones de la imagen. \n",
    "Después se combinan los 4 feature maps para generar el prior global, y este se concatena con el feature map original, y se pasa por una capa de convolución para generar el mapa de predicción final.\n",
    "\n",
    "![pspnet](https://production-media.paperswithcode.com/methods/new_pspnet-eps-converted-to.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(X,filters):    \n",
    "    f1,f2,f3 = filters\n",
    "    X_skip = X\n",
    "\n",
    "    # block_a\n",
    "    X = tf.keras.layers.Convolution2D(filters=f1, kernel_size=(1,1), dilation_rate=(1,1), padding='same', kernel_initializer='he_normal')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.ReLU()(X)\n",
    "\n",
    "    # block_b\n",
    "    X = tf.keras.layers.Convolution2D(filters=f2, kernel_size=(3,3), dilation_rate=(2,2), padding='same', kernel_initializer='he_normal')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.ReLU()(X)\n",
    "\n",
    "    # block_c\n",
    "    X = tf.keras.layers.Convolution2D(filters=f3, kernel_size=(1,1), dilation_rate=(1,1), padding='same', kernel_initializer='he_normal')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "\n",
    "    # skip_conv\n",
    "    X_skip = tf.keras.layers.Convolution2D(filters=f3, kernel_size=(3,3), padding='same', kernel_initializer='he_normal')(X_skip)\n",
    "    X_skip = tf.keras.layers.BatchNormalization()(X_skip)\n",
    "\n",
    "    # block_c + skip_conv\n",
    "    X = tf.keras.layers.Add()([X,X_skip])\n",
    "    X = tf.keras.layers.ReLU()(X)\n",
    "    \n",
    "    return X\n",
    "    \n",
    "def base_feature_maps(input_layer):\n",
    "    # block_1\n",
    "    base = conv_block(input_layer,[32,32,64])\n",
    "\n",
    "    # block_2\n",
    "    base = conv_block(base,[64,64,128])\n",
    "\n",
    "    # block_3\n",
    "    base = conv_block(base,[128,128,256])\n",
    "\n",
    "    return base\n",
    "\n",
    "def pyramid_feature_maps(input_layer):\n",
    "    base = base_feature_maps(input_layer)\n",
    "\n",
    "    # red\n",
    "    red = tf.keras.layers.GlobalAveragePooling2D()(base)\n",
    "    red = tf.keras.layers.Reshape((1,1,256))(red)\n",
    "    red = tf.keras.layers.Convolution2D(filters=64, kernel_size=(1,1))(red)\n",
    "    red = tf.keras.layers.UpSampling2D(size=128, interpolation='bilinear')(red)\n",
    "\n",
    "    # yellow\n",
    "    yellow = tf.keras.layers.AveragePooling2D(pool_size=(2,2))(base)\n",
    "    yellow = tf.keras.layers.Convolution2D(filters=64, kernel_size=(1,1))(yellow)\n",
    "    yellow = tf.keras.layers.UpSampling2D(size=2, interpolation='bilinear')(yellow)\n",
    "\n",
    "    # blue\n",
    "    blue = tf.keras.layers.AveragePooling2D(pool_size=(4,4))(base)\n",
    "    blue = tf.keras.layers.Convolution2D(filters=64, kernel_size=(1,1))(blue)\n",
    "    blue = tf.keras.layers.UpSampling2D(size=4, interpolation='bilinear')(blue)\n",
    "\n",
    "    # green\n",
    "    green = tf.keras.layers.AveragePooling2D(pool_size=(8,8))(base)\n",
    "    green = tf.keras.layers.Convolution2D(filters=64, kernel_size=(1,1))(green)\n",
    "    green = tf.keras.layers.UpSampling2D(size=8, interpolation='bilinear')(green)\n",
    "\n",
    "    return tf.keras.layers.concatenate([base,red,yellow,blue,green])\n",
    "\n",
    "def last_conv_module(input_layer, num_classes):\n",
    "    x = pyramid_feature_maps(input_layer)\n",
    "    x = tf.keras.layers.Convolution2D(filters=3, kernel_size=3, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    outputs = tf.keras.layers.Conv2D(num_classes, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"softmax\")(x)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def pspnet(input_shape, num_classes):\n",
    "    input_layer = tf.keras.layers.Input(shape = input_shape)\n",
    "    output_layer = last_conv_module(input_layer, num_classes)\n",
    "    model = tf.keras.Model(inputs=input_layer,outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pspnet(input_shape=X_train.shape[1:], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = tf.keras.optimizers.Adam(), metrics = [\"accuracy\", UpdatedMeanIoU(num_classes=num_classes, name = \"mean_iou\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience = 10, restore_best_weights = True)\n",
    "viz_callback = VizCallback(\"cityscapes_data/val/106.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train, y=Y_train, epochs = 1, batch_size = 16, validation_data = (X_valid, Y_valid), callbacks=[early_stopping, viz_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_preds(model, id_map):\n",
    "    plt.style.use(\"default\")\n",
    "    for i in os.listdir(\"cityscapes_data/val/\")[:3]:\n",
    "        img, mask = preprocess(f\"cityscapes_data/val/{i}\", id_map)\n",
    "        img = np.array(img)\n",
    "        img = np.reshape(img, (1, 128, 128, 3))\n",
    "        pred = model.predict(img)\n",
    "        y_pred = tf.math.argmax(pred, axis=-1)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_pred = np.reshape(y_pred, (128, 128))\n",
    "        fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize=(10, 5))\n",
    "        img = np.reshape(img, (128, 128, 3))\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(mask, cmap=\"viridis\")\n",
    "        axes[1].set_title(\"Original Mask\")\n",
    "        axes[1].axis(\"off\")\n",
    "        axes[2].imshow(y_pred, cmap=\"viridis\") \n",
    "        axes[2].set_title(\"Predicted Mask\")\n",
    "        axes[2].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_preds(model, id_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 U-NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net [https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28] es una arquitectura para segmentación semántica. Consta de una rama de contracción y una rama de expansión. La ruta de contratación sigue la arquitectura típica de una red convolucional. Consiste en la aplicación repetida de dos convoluciones de 3x3, cada una seguida por una ReLU y una operación de max pooling de 2x2 con stride 2 para reducir la resolución. \n",
    "\n",
    "En cada paso de reducción de resolución duplicamos el número de canales. Cada paso en la ruta expansiva consiste en un muestreo superior del feature map, seguido de un upsampling2D de 2x2 que reduce a la mitad el número de canales de características, despues mediante skip connections concatenamos las capas residuales correspondientes y dos convoluciones 3x3, cada una seguida de una ReLU. En la capa final, se utiliza una convolución de 1x1 para asignar cada pixel al número deseado de clases\n",
    "\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png\" alt=\"unet\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_model(img_shape, num_classes):\n",
    "    #Capa de Input\n",
    "    inputs = tf.keras.layers.Input(shape = img_shape)\n",
    "\n",
    "    #Primer Downsample\n",
    "    feature_map_1 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation='relu')(inputs)\n",
    "    feature_map_1 = tf.keras.layers.BatchNormalization()(feature_map_1)\n",
    "    feature_map_1 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(feature_map_1) #Usar despues en residual connection 4\n",
    "    maxpooling_1 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(feature_map_1)\n",
    "    maxpooling_1 = tf.keras.layers.Dropout(0.2)(maxpooling_1)\n",
    "\n",
    "    #Segundo Downsample\n",
    "    feature_map_2 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(maxpooling_1)\n",
    "    feature_map_2 = tf.keras.layers.BatchNormalization()(feature_map_2)\n",
    "    feature_map_2 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(feature_map_2)\n",
    "    maxpooling_2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(feature_map_2)\n",
    "    maxpooling_2 = tf.keras.layers.Dropout(0.2)(maxpooling_2)\n",
    "\n",
    "    #Tercer Downsample\n",
    "    feature_map_3 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(maxpooling_2)\n",
    "    feature_map_3 = tf.keras.layers.BatchNormalization()(feature_map_3)\n",
    "    feature_map_3 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(feature_map_3)\n",
    "    maxpooling_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(feature_map_3)\n",
    "    maxpooling_3 = tf.keras.layers.Dropout(0.2)(maxpooling_3)\n",
    "\n",
    "    #Cuarto Downsample\n",
    "    feature_map_4 = tf.keras.layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(maxpooling_3)\n",
    "    feature_map_4 = tf.keras.layers.BatchNormalization()(feature_map_4)\n",
    "    feature_map_4 = tf.keras.layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(feature_map_4)\n",
    "    maxpooling_4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(feature_map_4)\n",
    "    maxpooling_4 = tf.keras.layers.Dropout(0.2)(maxpooling_4)\n",
    "\n",
    "    #Quinto Downsample\n",
    "    feature_map_5 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(maxpooling_4)\n",
    "    feature_map_5 = tf.keras.layers.BatchNormalization()(feature_map_5)\n",
    "    feature_map_5 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(feature_map_5) \n",
    "\n",
    "    #Primer Upsample\n",
    "    upsample_1 = tf.keras.layers.UpSampling2D(size = (2, 2))(feature_map_5)\n",
    "    upsample_1 = tf.keras.layers.Dropout(0.2)(upsample_1)\n",
    "    upsample_1 = tf.keras.layers.Concatenate()([upsample_1, feature_map_4]) #Residual Connection 1\n",
    "    upsample_1 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1 ,activation = \"relu\")(upsample_1)\n",
    "    upsample_1 = tf.keras.layers.BatchNormalization()(upsample_1)\n",
    "    upsample_1 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(upsample_1)\n",
    "\n",
    "    #Segundo Upsample\n",
    "    upsample_2 = tf.keras.layers.UpSampling2D(size=(2, 2))(upsample_1)\n",
    "    upsample_2 = tf.keras.layers.Dropout(0.2)(upsample_2)\n",
    "    upsample_2 = tf.keras.layers.Concatenate()([upsample_2, feature_map_3])\n",
    "    upsample_2 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(upsample_2)\n",
    "    upsample_2 = tf.keras.layers.BatchNormalization()(upsample_2)\n",
    "    upsample_2 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(upsample_2)\n",
    "\n",
    "    #Tercer Upsample\n",
    "    upsample_3 = tf.keras.layers.UpSampling2D(size=(2, 2))(upsample_2)\n",
    "    upsample_3 = tf.keras.layers.Dropout(0.2)(upsample_3)\n",
    "    upsample_3 = tf.keras.layers.Concatenate()([upsample_3, feature_map_2])\n",
    "    upsample_3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(upsample_3)\n",
    "    upsample_3 = tf.keras.layers.BatchNormalization()(upsample_3)\n",
    "    upsample_3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(upsample_3)\n",
    "\n",
    "\n",
    "    #Cuarto Upsample\n",
    "    upsample_4 = tf.keras.layers.UpSampling2D(size=(2, 2))(upsample_3)\n",
    "    upsample_4 = tf.keras.layers.Dropout(0.2)(upsample_4)\n",
    "    upsample_4 = tf.keras.layers.Concatenate()([upsample_4, feature_map_1])\n",
    "    upsample_4 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(upsample_4)\n",
    "    upsample_4 = tf.keras.layers.BatchNormalization()(upsample_4)\n",
    "    upsample_4 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\", strides=1, activation=\"relu\")(upsample_4)\n",
    "\n",
    "\n",
    "    #Capa de Salida\n",
    "    outputs = tf.keras.layers.Conv2D(num_classes, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"softmax\")(upsample_4)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T07:46:12.239494Z",
     "iopub.status.busy": "2022-09-21T07:46:12.239062Z",
     "iopub.status.idle": "2022-09-21T07:46:12.497871Z",
     "shell.execute_reply": "2022-09-21T07:46:12.496762Z",
     "shell.execute_reply.started": "2022-09-21T07:46:12.239457Z"
    },
    "id": "C3YHi49qkhZX"
   },
   "outputs": [],
   "source": [
    "model = get_unet_model(img_shape=X_train.shape[1:], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.1 Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "unet_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "unet_metrics = [\"accuracy\", UpdatedMeanIoU(num_classes=num_classes, name=\"mean_iou\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T07:46:15.294393Z",
     "iopub.status.busy": "2022-09-21T07:46:15.293206Z",
     "iopub.status.idle": "2022-09-21T07:46:15.311034Z",
     "shell.execute_reply": "2022-09-21T07:46:15.309923Z",
     "shell.execute_reply.started": "2022-09-21T07:46:15.294322Z"
    },
    "id": "61WBRNghkhZX"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = unet_loss, optimizer = unet_opt, metrics = unet_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.2 Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T07:46:15.823381Z",
     "iopub.status.busy": "2022-09-21T07:46:15.822988Z",
     "iopub.status.idle": "2022-09-21T07:46:15.829143Z",
     "shell.execute_reply": "2022-09-21T07:46:15.827969Z",
     "shell.execute_reply.started": "2022-09-21T07:46:15.823332Z"
    },
    "id": "Vmhpxh--khZX"
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience = 10, restore_best_weights = True)\n",
    "viz_callback = VizCallback(\"cityscapes_data/val/106.jpg\")\n",
    "unet_callbacks = [early_stopping, viz_callback] # Utilizar los callbacks de early stopping y viz_callback en una lista para pasarselos al modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kBOCiZUkhZX"
   },
   "source": [
    "#### 2.7.3 Entrenando U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T07:46:17.054602Z",
     "iopub.status.busy": "2022-09-21T07:46:17.054218Z",
     "iopub.status.idle": "2022-09-21T08:04:49.131560Z",
     "shell.execute_reply": "2022-09-21T08:04:49.130605Z",
     "shell.execute_reply.started": "2022-09-21T07:46:17.054568Z"
    },
    "id": "S0dSq1TDkhZX",
    "outputId": "2e054614-4f47-4536-ca7c-057b620a2e16"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train, y=Y_train, epochs = 100, batch_size = 16, validation_data = (X_valid, Y_valid), callbacks=unet_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F61VQdYQkhZY"
   },
   "source": [
    "### 2.8 Resultados del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T08:12:48.566342Z",
     "iopub.status.busy": "2022-09-21T08:12:48.565932Z",
     "iopub.status.idle": "2022-09-21T08:12:49.244558Z",
     "shell.execute_reply": "2022-09-21T08:12:49.243540Z",
     "shell.execute_reply.started": "2022-09-21T08:12:48.566301Z"
    },
    "id": "rIE33msfkhZY",
    "outputId": "39e74571-b7e9-4ef8-e168-6439319e23ca"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz0HhNZIkhZY"
   },
   "source": [
    "### 2.9 Prediccion en el Set de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T08:22:30.730040Z",
     "iopub.status.busy": "2022-09-21T08:22:30.729643Z",
     "iopub.status.idle": "2022-09-21T08:22:45.628842Z",
     "shell.execute_reply": "2022-09-21T08:22:45.627874Z",
     "shell.execute_reply.started": "2022-09-21T08:22:30.730007Z"
    },
    "id": "l9LABUHnkhZY",
    "outputId": "6e477730-e313-4866-b984-b09e30da4e3a"
   },
   "outputs": [],
   "source": [
    "def plot_validation_preds(model, id_map):\n",
    "    plt.style.use(\"default\")\n",
    "    for i in os.listdir(\"cityscapes_data/val/\")[:3]:\n",
    "        img, mask = preprocess(f\"cityscapes_data/val/{i}\", id_map)\n",
    "        img = np.array(img)\n",
    "        img = np.reshape(img, (1, 128, 128, 3))\n",
    "        pred = model.predict(img)\n",
    "        y_pred = tf.math.argmax(pred, axis=-1)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_pred = np.reshape(y_pred, (128, 128))\n",
    "        fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize=(10, 5))\n",
    "        img = np.reshape(img, (128, 128, 3))\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(mask, cmap=\"viridis\")\n",
    "        axes[1].set_title(\"Original Mask\")\n",
    "        axes[1].axis(\"off\")\n",
    "        axes[2].imshow(y_pred, cmap=\"viridis\") \n",
    "        axes[2].set_title(\"Predicted Mask\")\n",
    "        axes[2].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_preds(model, id_map)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
